{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxIMgTgY38iSNVP/cg0Q0f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrutakeerti/Twitter-Post-Sentiment-Analysis/blob/main/untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Step 1: Data Collection and Preprocessing\n",
        "# Dummy dataset\n",
        "data = {\n",
        "    'post': [\n",
        "        'I love this product!',\n",
        "        'This is the worst service ever.',\n",
        "        'Amazing experience, will come again.',\n",
        "        'I am very disappointed.',\n",
        "        'Totally worth it!',\n",
        "        'Never coming back here.'\n",
        "    ],\n",
        "    'reply': [\n",
        "        'Me too, it’s fantastic!',\n",
        "        'I agree, it’s terrible.',\n",
        "        'Absolutely wonderful!',\n",
        "        'Same here, very let down.',\n",
        "        'Indeed, it’s great!',\n",
        "        'Terrible experience.'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', str(text))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df['post'] = df['post'].apply(preprocess_text)\n",
        "df['reply'] = df['reply'].apply(preprocess_text)\n",
        "\n",
        "# Combine post and reply for analysis\n",
        "df['post_reply'] = df['post'] + ' ' + df['reply']\n",
        "\n",
        "# Step 2: Feature Extraction\n",
        "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X = vectorizer.fit_transform(df['post_reply']).toarray()\n",
        "\n",
        "# Step 3: Autoencoder Model\n",
        "input_dim = X.shape[1]\n",
        "encoding_dim = 128\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(X, X, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
        "\n",
        "# Extract the encoder part for dimensionality reduction\n",
        "encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
        "encoded_X = encoder_model.predict(X)\n",
        "\n",
        "# Step 4: Clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "clusters = kmeans.fit_predict(encoded_X)\n",
        "\n",
        "# Assign clusters to the original data\n",
        "df['cluster'] = clusters\n",
        "\n",
        "# Step 5: Evaluation\n",
        "silhouette_avg = silhouette_score(encoded_X, clusters)\n",
        "print(f'Silhouette Score: {silhouette_avg}')\n",
        "\n",
        "# Display the clustered data\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh8LHVqyWNAw",
        "outputId": "0795b6f4-5392-4d6b-e718-a92345f750c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6908 - val_loss: 0.6883\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.6859 - val_loss: 0.6853\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.6809 - val_loss: 0.6823\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.6761 - val_loss: 0.6793\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.6712 - val_loss: 0.6763\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.6664 - val_loss: 0.6733\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.6616 - val_loss: 0.6704\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.6568 - val_loss: 0.6675\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.6520 - val_loss: 0.6646\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.6473 - val_loss: 0.6617\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.6425 - val_loss: 0.6588\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.6377 - val_loss: 0.6558\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.6328 - val_loss: 0.6529\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.6280 - val_loss: 0.6500\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.6231 - val_loss: 0.6471\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.6182 - val_loss: 0.6441\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.6132 - val_loss: 0.6411\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.6082 - val_loss: 0.6381\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.6031 - val_loss: 0.6351\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.5980 - val_loss: 0.6321\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5928 - val_loss: 0.6290\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.5876 - val_loss: 0.6259\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5824 - val_loss: 0.6228\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.5771 - val_loss: 0.6197\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.5717 - val_loss: 0.6165\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5662 - val_loss: 0.6132\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.5607 - val_loss: 0.6099\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.5552 - val_loss: 0.6065\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.5495 - val_loss: 0.6031\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.5439 - val_loss: 0.5997\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.5381 - val_loss: 0.5962\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.5324 - val_loss: 0.5927\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.5265 - val_loss: 0.5892\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.5206 - val_loss: 0.5856\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.5146 - val_loss: 0.5820\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.5086 - val_loss: 0.5784\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.5026 - val_loss: 0.5747\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.4965 - val_loss: 0.5710\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.4904 - val_loss: 0.5673\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.4842 - val_loss: 0.5635\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.4781 - val_loss: 0.5597\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.4719 - val_loss: 0.5559\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.4656 - val_loss: 0.5521\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.4594 - val_loss: 0.5483\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.4531 - val_loss: 0.5445\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.4469 - val_loss: 0.5406\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.4407 - val_loss: 0.5367\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4344 - val_loss: 0.5329\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4282 - val_loss: 0.5290\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.4219 - val_loss: 0.5252\n",
            "1/1 [==============================] - 0s 285ms/step\n",
            "Silhouette Score: 0.11824134737253189\n",
            "                                 post                    reply  \\\n",
            "0                 i love this product    me too it s fantastic   \n",
            "1      this is the worst service ever    i agree it s terrible   \n",
            "2  amazing experience will come again     absolutely wonderful   \n",
            "3              i am very disappointed  same here very let down   \n",
            "4                    totally worth it        indeed it s great   \n",
            "5              never coming back here      terrible experience   \n",
            "\n",
            "                                          post_reply  cluster  \n",
            "0          i love this product me too it s fantastic        1  \n",
            "1  this is the worst service ever i agree it s te...        0  \n",
            "2  amazing experience will come again absolutely ...        0  \n",
            "3     i am very disappointed same here very let down        0  \n",
            "4                 totally worth it indeed it s great        0  \n",
            "5         never coming back here terrible experience        0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}